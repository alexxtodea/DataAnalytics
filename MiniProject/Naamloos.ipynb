{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d990dc6",
   "metadata": {},
   "source": [
    "# UCI SEMICOM dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c98f2e",
   "metadata": {},
   "source": [
    "After doing research about the dataset which can be found in the *word document* I will have put in the same folder as this analysis, I will now start to work on the dataset. I've taken a look into the dataset ( which you can also see in the sample ) and I know I have many columns with numerical variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c81b16c",
   "metadata": {},
   "source": [
    "#### First we import all the important stuff and our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008e2343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "SemiCom = pd.read_csv(\"uci-secom.csv\")\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe2572",
   "metadata": {},
   "source": [
    "#### I will also add a function that makes sure the output is shown on full screen and not in a scrollable block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1b5450",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "return false;\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa408dab",
   "metadata": {},
   "source": [
    "Lets first see the size of our dataset. As we can see below we have to deal with a pretty big dataset. \n",
    "Now lets look a little closer to see what type of data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f043293",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SemiCom.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "720a356e",
   "metadata": {},
   "source": [
    "mentions something about the nature of these columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b6e46c",
   "metadata": {},
   "source": [
    "***\n",
    "Here we can check our data really quick.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee6735c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SemiCom.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd0c0bca",
   "metadata": {},
   "source": [
    "### A small explanation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dee9f28",
   "metadata": {},
   "source": [
    "***\n",
    "By looking at the sample and reading on kaggle about this dataset I will explain to you what this is.\n",
    "This dataset is information about a machine with alot of sensors ( about 600 of them ). These sensors have an output which is always numerical or NaN. Next to that there are \n",
    "two other columns which are: Time, and pass/fail. Our goal is to predict to the best of our abilities if a row will pass or fail by using the most important features of the sensors.\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fb0a8c7",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2265b3",
   "metadata": {},
   "source": [
    "***\n",
    "Before I can start to work with this dataset i need to clean it. The information from the dataset said we did have missing values so lets start to work on those:\n",
    "\n",
    "* I first want to see what I'm dealing with. So I can decide if i want to remove columns or add values.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab403a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check how many rows and columns we have in this dataset\n",
    "totaldata = np.product(SemiCom.shape)\n",
    "totaldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d0acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total amount of missimg data\n",
    "missingdata = SemiCom.isnull().sum()\n",
    "totalmissingdata = missingdata.sum()\n",
    "totalmissingdata "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789d19a8",
   "metadata": {},
   "source": [
    "***\n",
    "Now one thing i want to do is check the percentage of the total missing values in this dataset.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c5662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(totalmissingdata/totaldata) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183554b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wanted to check which colums had the most NaN values\n",
    "aa = missingdata.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e52fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07e33fb2",
   "metadata": {},
   "source": [
    "comment: can you study where these missing values occur, are they co-occuring look like they are, can you seperate those rows, and analyze them separately. \n",
    "also you can check with the data description if there is any statemetn that may explain the missing values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42aceca9",
   "metadata": {},
   "source": [
    "# Step 1 Cleaning: The threshold ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37db187f",
   "metadata": {},
   "source": [
    "***\n",
    "As you can see not much of the data is missing so removing these wont have a big impact since the dataset has very many values. But, it is necessary to have a clean dataset so that our prediction is more accurate. So my plan is to make a threshold of 15%. When a column is missing more then 15% the collumn gets removed.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e648f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.15\n",
    "\n",
    "columns_to_drop = missingdata[missingdata > threshold * len(SemiCom)].index\n",
    "print(columns_to_drop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe1158a6",
   "metadata": {},
   "source": [
    "***\n",
    "So here we can see all the columns that are above the threshold and need to be removed. My next step is dropping these columns and checking before and after if columns have been removed. I wanted to do this bit with the 'dropna()' function but this drops rows or columns based on missing values. It cannot be used to drop columns that you specify.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91722be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SemiCom.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47704a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "SemiCom_dropped = SemiCom.drop(columns=columns_to_drop) #Dropping the columns that have more then 15% missing values\n",
    "\n",
    "print(SemiCom_dropped.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a2e4aba",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "As you can see our columns length has gone down from 592 to 540. Now we need look for other ways to remove columns that are useless because now we still have too many columns. \n",
    "After doing some research and asking ChatGPT how i could clean a dataset that has many numerical columns. I found the Variance threshold which means that you remove the columns that have mostly the same information. And because it's almost constantly the same it is not very usefull.\n",
    "\n",
    "This is usefull for me because my dataset has many columns with probably the same information. Which wont provide any extra information for the model.\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9698b34",
   "metadata": {},
   "source": [
    "# Step 2: Variance threshold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cc41660",
   "metadata": {},
   "source": [
    "*** \n",
    "First we have to make sure we have the types to a numerical type. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#had a small error about the time not being able to covert to float and because time is not usefull to the model, I removed it\n",
    "\n",
    "SemiCom_dropped= SemiCom_dropped.drop(['Time'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75910ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SemiCom_dropped.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "805927ab",
   "metadata": {},
   "source": [
    "***\n",
    "I chose a variance of 0.05 because in my opinion if its beneath 0.05 it is a very minimal change and wont affect the module.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead622dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thresholder = VarianceThreshold(threshold=0.05)\n",
    "\n",
    "X_high_variance = thresholder.fit_transform(SemiCom_dropped)\n",
    "#put the remaining columns in a list\n",
    "selected_features = SemiCom_dropped.columns[thresholder.get_support()].tolist()\n",
    "SemiCom_filtered = SemiCom_dropped[selected_features]\n",
    "\n",
    "SemiCom_filtered.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "408df688",
   "metadata": {},
   "source": [
    "***\n",
    "Yep! that was a good one. We just cut our columns in half from 540 to 251.\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "682785eb",
   "metadata": {},
   "source": [
    "# Step 3: Correlation Matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06de58ca",
   "metadata": {},
   "source": [
    "***\n",
    "With this method we want to reduce highly correlated columns. The reason behind this is that we probably have columns that have similar information. This will help minimalize the dataset and gives us more relevant information.\n",
    "\n",
    "I will be using a threshold of 0.8 which means that any columns with a correlation above 0.8 will be added to the list of columns that will be removed. The \"For\" loop compares all the columns with eachother and if the columns are highly correlated they will be put into the variable \"i\" and \"j\". all the columns in \"j\" will be removed.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615183ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correlation_matrix = SemiCom_filtered.corr().abs()\n",
    "threshold = 0.8  #Remove columns with a correlation above 0.8\n",
    "\n",
    "#Find columns with high correlation\n",
    "highly_correlated_cols = []\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if correlation_matrix.iloc[i, j] > threshold:\n",
    "            colname_i = correlation_matrix.columns[i]\n",
    "            colname_j = correlation_matrix.columns[j]\n",
    "            highly_correlated_cols.append(colname_j)\n",
    "\n",
    "#Dropping highly correlated columns\n",
    "SemiCom_correlated = SemiCom_filtered.drop(columns=highly_correlated_cols)\n",
    "print(highly_correlated_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b3f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SemiCom_correlated.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd3146ea",
   "metadata": {},
   "source": [
    "***\n",
    "Now we're getting somewhere. We went from 251 to 138. I think this is pretty decent but I'm not completely happy with the amount of columns left. So I'm going to do some more research and find other ways to minimalize this number.\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45e80384",
   "metadata": {},
   "source": [
    "# Step 4: Fill Missing values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0882a4a4",
   "metadata": {},
   "source": [
    "***\n",
    "One of the last Cleaning steps is filling the remaining missing values. I'm doing this by filling the missing value with the mean or median. But which one is the best option for this dataset? When I searched online I found out that mean is often used when the distribution is pretty symmetric in this case median can also be used. The difference wont be big. When the distribution is skewed the mean is not useful. The median is less sensitive to outliers.\n",
    "\n",
    "So our first step is seeing what distribution this dataset has.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a4504",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlationmat = SemiCom_correlated.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(np.abs(correlationmat), vmax=.8, square=True);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93eb1bd3",
   "metadata": {},
   "source": [
    "is target here or not. make it clear!\n",
    "\n",
    "[optional] Try out Hierarchical clustering and find similar columns, maybe also useful for missing values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2a165ab",
   "metadata": {},
   "source": [
    "***\n",
    "Let's look at the skewness of our dataset.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137eb9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = SemiCom_correlated.skew()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(skewness, bins=20)\n",
    "plt.title('Skewness')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e1beb5b",
   "metadata": {},
   "source": [
    "the tail values are important to identify , maybe you want to remove these or note any pattern, later if you build a model, you can check if it works on these points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed43aa20",
   "metadata": {},
   "source": [
    "***\n",
    "So from the graph above we can mke the conclusion that we're dealing with a skewed distribution. And from our research we now know we need to use the \"Mean\" to fill/Impude the remaining missing values.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b034a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "missingvalues = SemiCom_correlated.isnull().sum()\n",
    "print(missingvalues.sum())\n",
    "print(missingvalues.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce4e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "SemiCom_correlated = SemiCom_correlated.fillna(SemiCom_correlated.mean())\n",
    "missingdata = SemiCom_correlated.isnull().sum()\n",
    "totalmissingdata = missingdata.sum()\n",
    "print(totalmissingdata)\n",
    "print(missingdata.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c55c602",
   "metadata": {},
   "source": [
    "build a base line early, "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c523184",
   "metadata": {},
   "source": [
    "# Step 5: Handling outliers - Is this a good method for my dataset?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7378e833",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "To clean my dataset some more my next step was to get rid of the outliers. But when i started removing the outliers 1/3 of my dataset went away. So I'm not too sure about handling the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6251fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = SemiCom_correlated.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94f509ad",
   "metadata": {},
   "source": [
    "optional: after modeling you can look into regions where the error is large, by selecting points with large prediction error, and compare the statisitcs of the features for those points with the the rest of the points. for example side to side bar plots for each feature. make a plot of the difference of the mean or median\n",
    "\n",
    "beacuae you have too many features, maybe you can group columns via hierarchical clustering and then analyze in the group.\n",
    "\n",
    "consider the model you want to use, does this model need scaling?\n",
    "\n",
    "\n",
    "calculate the correlation of all features with the target and make a bar plot, and find highly occrelating features, for those make a scatter plot. \n",
    "calculate the mutual information  of all features with the target and make a bar plot, and find highly occrelating features, for those make a scatter plot. \n",
    "\n",
    "goal is to predict ==. strategy is what is informaing us about the target\n",
    "\n",
    "compare performance as you inisitall planned. histograms.\n",
    "compare the count of each class, establish if it is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9577067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SemiCom_correlated.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_percentile = 99  \n",
    "\n",
    "\n",
    "for column in SemiCom_correlated.columns:\n",
    "    threshold = np.percentile(SemiCom_correlated[column], threshold_percentile)\n",
    "    SemiCom_correlated.loc[SemiCom_correlated[column] > threshold, column] = np.nan\n",
    "df_cleaned = SemiCom_correlated.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b9f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0907169",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cleaned.shape)\n",
    "print(SemiCom_correlated.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "SemiCom_correlated = SemiCom_correlated.fillna(SemiCom_correlated.mean())\n",
    "missing_values = SemiCom_correlated.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd859c6b",
   "metadata": {},
   "source": [
    "***\n",
    "Now we look at the accuracy of our Dataset when it is not cleared from outliers, and then we look at the accuracy of the dataset that will be cleaned of outliers. With this we can conclude if we need to remove outliers. And if not there is a big question in why there are so many outliers.\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f91131c9",
   "metadata": {},
   "source": [
    "### Uncleaned from outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5aeb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = SemiCom_correlated.drop(columns=['Pass/Fail'])\n",
    "y = SemiCom_correlated['Pass/Fail']\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(solver='sag', max_iter=6000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the original test set\n",
    "y_pred_original = model.predict(X_test)\n",
    "print(\"Accuracy: \", model.score(X_test,y_test)*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "501c115b",
   "metadata": {},
   "source": [
    "***\n",
    "here we split the dataset into train and test. now we have an accuracy but lets look at the correlation matrix.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbffa59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=1, max_iter=6000, solver='sag')\n",
    "lr.fit(X_train, y_train) \n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5126cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.rcParams['figure.figsize'] = (5, 5)\n",
    "sns.set(style = 'dark', font_scale = 1.4)\n",
    "sns.heatmap(cm, annot = True, annot_kws = {\"size\": 15})\n",
    "print(cm)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f519a273",
   "metadata": {},
   "source": [
    "***\n",
    "To summarize:\n",
    "\n",
    "- True Positives (TP): 288\n",
    "- False Positives (FP): 2\n",
    "- False Negatives (FN): 24\n",
    "- True Negatives (TN): 0\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", lr.score(X_test,y_test)*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned from outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b035801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing here as above but now with the outliers dropped.\n",
    "X = df_cleaned.drop(columns=['Pass/Fail'])\n",
    "y = df_cleaned['Pass/Fail']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression(solver='sag', max_iter=8000)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_original = model.predict(X_test)\n",
    "print(\"Accuracy: \", model.score(X_test,y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540cd930",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=1, solver='sag', max_iter=8000)\n",
    "lr.fit(X_train, y_train) \n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.rcParams['figure.figsize'] = (5, 5)\n",
    "sns.set(style = 'dark', font_scale = 1.4)\n",
    "sns.heatmap(cm, annot = True, annot_kws = {\"size\": 15})\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3487de0",
   "metadata": {},
   "source": [
    "***\n",
    "To summarize:\n",
    "\n",
    "- True Positives (TP): 92\n",
    "- False Positives (FP): 2\n",
    "- False Negatives (FN): 9\n",
    "- True Negatives (TN): 0\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50b47ce4",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "The confusion matrix can calculate 3 things for us:\n",
    "\n",
    "- Accuracy -> correct predictions/ total number of predictions\n",
    "- Precision -> TN / (TP + FP)\n",
    "- Recall -> TP / (FP + FN)\n",
    "\n",
    "When i calculate these for both confusion matrixes this is the outcome:\n",
    "\n",
    "Without outliers:\n",
    "\n",
    "A : 90.19%\n",
    "P : 97.87%\n",
    "R : 91.09%\n",
    "\n",
    "With outliers\n",
    "\n",
    "A : 91.89%\n",
    "P : 99.31%\n",
    "R : 92.31%\n",
    "\n",
    "\n",
    "This shows that the performance and accuracy with outliers is higher then the one without outliers. So in conclusion we will not be removing outliers.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b95bfcda",
   "metadata": {},
   "source": [
    "# Step 6: hierarchical clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fea618c",
   "metadata": {},
   "source": [
    "-\n",
    "-\n",
    "-\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "556aeb28",
   "metadata": {},
   "source": [
    "# Step 7: Oversampler / Undersampler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "998ccfbf",
   "metadata": {},
   "source": [
    "from working on another problem I came to this usefull method that is very helpful for this dataset. Let me explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccead14",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = SemiCom_correlated['Pass/Fail'].value_counts()\n",
    "\n",
    "plt.bar(value_counts.index, value_counts.values)\n",
    "#show amount of 0,1\n",
    "for i, count in enumerate(value_counts.values):\n",
    "    plt.text(i, count + 0.5, str(count), ha='center')\n",
    "\n",
    "plt.xlabel('Result')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Fail/Pass')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74c2cbf4",
   "metadata": {},
   "source": [
    "***\n",
    "The difference in pass and fail is huge. Which is why our model never guesses the \"negative\". A solution to this problem is using the oversampler. With this we can equallize the amount of the sample when it comes to pass and fail. Then the model should be able to predict better then now. So let's test that out.\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1afdd77",
   "metadata": {},
   "source": [
    "### Let's test if this is usefull for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30aec113",
   "metadata": {},
   "source": [
    "### Oversampled results: Precision, recall, accuracy\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa627e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "classifier = LogisticRegression(solver='sag', max_iter=9000)\n",
    "classifier.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(cm)\n",
    "sns.heatmap(confusion, annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3acef9ab",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "The reason we're using this is to look at the difference between the majority class (pass) and the minority class (fail). As we can see the minority is pretty bad. There is a majority in fale negatives.\n",
    "Now we are going to equalize the samples to hopefully change the performance.\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7665c7a2",
   "metadata": {},
   "source": [
    "### Undersampler: Creating equal samples to test this theorie\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695d850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "X_train_undersampled, y_train_undersampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "classifier.fit(X_train_undersampled, y_train_undersampled)\n",
    "\n",
    "y_pred = classifier.predict(X_train_undersampled)\n",
    "print(classification_report(y_train_undersampled, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39d3b75b",
   "metadata": {},
   "source": [
    "### This one is undersampled: (Equalized)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_train_undersampled)\n",
    "cmequal = confusion_matrix(y_train_undersampled, y_pred)\n",
    "sns.heatmap(cmequal, annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21811564",
   "metadata": {},
   "source": [
    "*** \n",
    "One thing i noticed here is that the the negatives are still very low. So to maybe get a better model i wanted to play around with the sample size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32ccf70b",
   "metadata": {},
   "source": [
    "## Sample_Stategy added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0aa750",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\n",
    "\n",
    "X_train_undersampled, y_train_undersampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "classifier.fit(X_train_undersampled, y_train_undersampled)\n",
    "\n",
    "y_pred = classifier.predict(X_train_undersampled)\n",
    "print(classification_report(y_train_undersampled, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_train_undersampled)\n",
    "cm = confusion_matrix(y_train_undersampled, y_pred)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb602d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc17d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cmequal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b1c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0df089f0",
   "metadata": {},
   "source": [
    "***\n",
    "This is pretty good because in our dataset false negatives are relatively better then false positives. because a fail that actually passed is better then the other way around.\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de723258",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003d92ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid =  {    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': [25, 30, 40, 50,54,53,52,55,56,57,60, 70, 80],\n",
    "    'min_samples_leaf': [1, 2, 3, 4 , 5 ,],}   \n",
    "# Create a decision tree classifier\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameter and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(best_params)\n",
    "print(best_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f00268c1",
   "metadata": {},
   "source": [
    "## Undersampled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98659fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid =  {    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': [5, 10 ,20,21, 22, 23 ,25, 30, 40, 50],\n",
    "    'min_samples_leaf': [1, 2, 3, 4 , 5 ,],}   \n",
    "# Create a decision tree classifier\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_undersampled, y_train_undersampled)\n",
    "\n",
    "# Get the best parameter and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(best_params)\n",
    "print(best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataAnalytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d2cf816009f15afbcb31a5581dd5e2ba87864b922c5f09b57520046109187d7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
