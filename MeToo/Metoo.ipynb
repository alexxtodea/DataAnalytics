{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/alex/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/alex/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import string\n",
    "\n",
    "\n",
    "mt = pd.read_csv('MeTooHate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.OutputArea.prototype._should_scroll = function(lines) {\nreturn false;\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "return false;\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To start:\n",
    "\n",
    "This dataset is too big for me to upload to GitHub. So, I cleaned the dataset first then I extracted the cleaned dataset and started a new notebook to continue the work. The reason behind this is because the cleaning takes a few minutes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Me Too Hate Comments\n",
    "***\n",
    "\n",
    "The goal of this project is to seperate hateful and non-hateful tweets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Cleaning the dataset\n",
    "\n",
    "which we always do by first taking a look at the big picture:\n",
    "- small view of the dataset\n",
    "- check the size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status_id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>location</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1046207313588236290</td>\n",
       "      <td>Entitled, obnoxious, defensive, lying weasel. ...</td>\n",
       "      <td>2018-09-30T01:17:15Z</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>McAllen, TX</td>\n",
       "      <td>2253</td>\n",
       "      <td>2303</td>\n",
       "      <td>23856</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1046207328113086464</td>\n",
       "      <td>Thank you  and  for what you did for the women...</td>\n",
       "      <td>2018-09-30T01:17:19Z</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Tampa, FL</td>\n",
       "      <td>2559</td>\n",
       "      <td>4989</td>\n",
       "      <td>19889</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1046207329589493760</td>\n",
       "      <td>Knitting (s) &amp;amp; getting ready for January 1...</td>\n",
       "      <td>2018-09-30T01:17:19Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>St Cloud, MN</td>\n",
       "      <td>16</td>\n",
       "      <td>300</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1046207341283168256</td>\n",
       "      <td>Yep just like triffeling women weaponized thei...</td>\n",
       "      <td>2018-09-30T01:17:22Z</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>flyover country</td>\n",
       "      <td>3573</td>\n",
       "      <td>3732</td>\n",
       "      <td>38361</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1046207347016826880</td>\n",
       "      <td>No, the President wants to end  movement posin...</td>\n",
       "      <td>2018-09-30T01:17:23Z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>World</td>\n",
       "      <td>294</td>\n",
       "      <td>312</td>\n",
       "      <td>7635</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             status_id                                               text  \\\n",
       "0  1046207313588236290  Entitled, obnoxious, defensive, lying weasel. ...   \n",
       "1  1046207328113086464  Thank you  and  for what you did for the women...   \n",
       "2  1046207329589493760  Knitting (s) &amp; getting ready for January 1...   \n",
       "3  1046207341283168256  Yep just like triffeling women weaponized thei...   \n",
       "4  1046207347016826880  No, the President wants to end  movement posin...   \n",
       "\n",
       "             created_at  favorite_count  retweet_count         location  \\\n",
       "0  2018-09-30T01:17:15Z               5              1      McAllen, TX   \n",
       "1  2018-09-30T01:17:19Z               5              2        Tampa, FL   \n",
       "2  2018-09-30T01:17:19Z               0              0     St Cloud, MN   \n",
       "3  2018-09-30T01:17:22Z               1              0  flyover country   \n",
       "4  2018-09-30T01:17:23Z               0              0            World   \n",
       "\n",
       "   followers_count  friends_count  statuses_count  category  \n",
       "0             2253           2303           23856         0  \n",
       "1             2559           4989           19889         0  \n",
       "2               16            300               9         0  \n",
       "3             3573           3732           38361         1  \n",
       "4              294            312            7635         0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(807174, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we've seen what the data is about and that we have a big dataset to work with.\n",
    "***\n",
    "Next step is looking for\n",
    "### missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status_id               0\n",
      "text                 3536\n",
      "created_at              0\n",
      "favorite_count          0\n",
      "retweet_count           0\n",
      "location           190768\n",
      "followers_count         0\n",
      "friends_count           0\n",
      "statuses_count          0\n",
      "category                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "IsNull = mt.isnull().sum()\n",
    "print(IsNull)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My plan cleaning:\n",
    "***\n",
    "We'll remove some of the useless columns first to get rid of useless information, \"Location\" is also going to be removed because it has too many missing values and is also not usefull for finding hate and non-hate comments. Next, we remove the isnull rows from the \"text\" because these are also not usefull for our model. Without any input those rows cannot help predict."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the code below we remove every column except \"Text\". All the other columns are irrelevant for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove useless columns\n",
    "mt = mt.drop(['status_id','location', 'created_at',\n",
    "        'followers_count', 'friends_count', 'statuses_count', 'category', 'retweet_count', 'favorite_count'\n",
    "       ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(803638, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt = mt.dropna()\n",
    "mt.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that looks a little smaller :)\n",
    "***\n",
    "Next up we use the NLT (Natural Language Toolkit) to clean off punctuation, stopwords. first step is cleaning the punctuations. \n",
    "\n",
    "### Cleaning punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Entitled obnoxious defensive lying weasel This...\n",
       "1         Thank you  and  for what you did for the women...\n",
       "2         Knitting s amp getting ready for January 19 20...\n",
       "3         Yep just like triffeling women weaponized thei...\n",
       "4         No the President wants to end  movement posing...\n",
       "                                ...                        \n",
       "807169    Let’s not forget that this “iconic kiss” was u...\n",
       "807170    DEFINITELYthe only one any of us should suppor...\n",
       "807171    Did the  movement count the dollars of Erin An...\n",
       "807172    This is one of my all time fav songs amp video...\n",
       "807173     I watched your news on the death of the sailo...\n",
       "Name: text_without_punct, Length: 803638, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text_without_punct = text.translate(translator)\n",
    "    return text_without_punct\n",
    "\n",
    "mt['text_without_punct'] = mt['text'].apply(remove_punctuation)\n",
    "mt['text_without_punct']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize the text here to break it down into individual words. This is very usefull when youre preprocessing text.\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "Next step\n",
    "### Removing stopwords\n",
    "\n",
    "\"Stop words are commonly used words (e.g., \"a\", \"an\", \"the\") that often do not carry significant meaning and are typically removed during text processing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Entitled obnoxious defensive lying weasel thin...\n",
       "1                                Thank women survivors week\n",
       "2                Knitting amp getting ready January 19 2019\n",
       "3         Yep like triffeling women weaponized poon Wond...\n",
       "4              President wants end movement posing movement\n",
       "                                ...                        \n",
       "807169    Let ’ forget “ iconic kiss ” uninvited sexual ...\n",
       "807170    DEFINITELYthe one us support unconditionally G...\n",
       "807171        movement count dollars Erin Andrews wondering\n",
       "807172    one time fav songs amp videos brutally honest ...\n",
       "807173    watched news death sailor famous WW2 Kiss phot...\n",
       "Name: text_without_stopwords, Length: 803638, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]  #.lower() -> lowercase\n",
    "    text_without_stopwords = ' '.join(filtered_tokens) #joins words back together in a string\n",
    "    return text_without_stopwords\n",
    "\n",
    "mt['text_without_stopwords'] = mt['text_without_punct'].apply(remove_stopwords)\n",
    "mt['text_without_stopwords'] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "To give you a better understanding ( and myself ) im going to give a small explanation on what these are and what they mean.\n",
    "\n",
    "Stemming:\n",
    "\n",
    "Stemming involves reducing words to their base or root form by removing suffixes or prefixes. It follows a set of predefined rules to transform words. For example, words like \"running,\" \"runs,\" and \"ran\" would all be reduced to their stem \"run.\" \n",
    "\n",
    "plural noun: suffixes\n",
    "1. a morpheme added at the end of a word to form a derivative (e.g. -ation, -fy, -ing, -itis ).\n",
    "\"for the last few decades we've appended the suffix ‘gate’ to basically any scandal\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization:\n",
    "Lemmatization is a more advanced technique compared to stemming. It aims to determine the base form of words using vocabulary and morphological analysis. For example, the word \"better\" would be lemmatized to \"good\" rather than the simple stem \"bet.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [entitl, obnoxi, defens, lie, weasel, thing, m...\n",
       "1                            [thank, woman, survivor, week]\n",
       "2                [knit, amp, get, readi, januari, 19, 2019]\n",
       "3         [yep, like, triffel, woman, weapon, poon, wond...\n",
       "4             [presid, want, end, movement, pose, movement]\n",
       "                                ...                        \n",
       "807169    [let, ’, forget, “, icon, kiss, ”, uninvit, se...\n",
       "807170    [definitelyth, one, u, support, uncondit, godi...\n",
       "807171      [movement, count, dollar, erin, andrew, wonder]\n",
       "807172    [one, time, fav, song, amp, video, brutal, hon...\n",
       "807173    [watch, news, death, sailor, famou, ww2, kiss,...\n",
       "Name: processed_text, Length: 803638, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Stemming\n",
    "    stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
    "    \n",
    "    return lemmatized_words\n",
    "\n",
    "mt['processed_text'] = mt['text_without_stopwords'].apply(preprocess_text)\n",
    "mt['processed_text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Now that we have cleaned the datset we will extract the cleaned \"mt\" and create a new dataset with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = pd.DataFrame({'processed_text': mt['processed_text']})\n",
    "new_dataset.to_csv('new_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataAnalytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
